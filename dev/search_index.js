var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = PPLM","category":"page"},{"location":"#PPLM.jl","page":"Home","title":"PPLM.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"PPLM.jl is a julia based implementation of Plug and Play Language Models. The implementation is primarily based on Transformers.jl GPT2 and allows user to steer the Text generation task based on some Attribute Models.","category":"page"},{"location":"#Why-PPLM.jl?","page":"Home","title":"Why PPLM.jl?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"While large pretrained language models can generate coherent text, it's hard to control the context are actually generating.  Plug and Play Language Models or PPLM allows a user to flexibly plug in one or more tiny attribute models representing the desired steering objective into a large, unconditional language model (LM). While the main feature of this package is to help with Controlled Text generation, it also facilitates the following through simple API functions: ","category":"page"},{"location":"","page":"Home","title":"Home","text":"GPT2 pretrained Tokenizer\nNormal Text generation with GPT2 using few lines of code.\nPretrained Discriminators from Huggingface loaded as BSON file. \nSome predefined BagofWords.\nDiscriminator Training -  Linear layer classifier on GPT2 model\nSome more options for Controlled generation of Text, as an extension to PPLM.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Will be updated once registered...","category":"page"},{"location":"","page":"Home","title":"Home","text":"Not yet regsitered","category":"page"}]
}
