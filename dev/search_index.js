var documenterSearchIndex = {"docs":
[{"location":"api/#API-Functions","page":"API","title":"API Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Here are some of the API functions provided with this package:","category":"page"},{"location":"api/#GPT2-Tokenizer","page":"API","title":"GPT2 Tokenizer","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"PPLM.load_pretrained_tokenizer\nPPLM.tokenize\nPPLM.encode\nPPLM.decode\nPPLM.detokenize","category":"page"},{"location":"api/#PPLM.load_pretrained_tokenizer","page":"API","title":"PPLM.load_pretrained_tokenizer","text":"load_pretrained_tokenizer(ty::Type{T}; unk_token=\"<|endoftext|>\", eos_token=\"<|endoftext|>\", pad_token=\"<|endoftext|>\") where T<:PretrainedTokenizer\n\nLoad GPT2 tokenizer using Datadeps for pretrained bpe and vocab. Returns tokenizer as GPT2Tokenizer structure.\n\n\n\n\n\nload_pretrained_tokenizer(path_bpe, path_vocab, unk_token, eos_token, pad_token)\n\nLoad pretrained tokenizer for GPT2 from provided bpe and vocab file path. Initialises unk_token, eos_token, pad_token as provided with the function. Returns tokenizer as GPT2Tokenizer structure.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.tokenize","page":"API","title":"PPLM.tokenize","text":"tokenize(t::GPT2Tokenizer, text::AbstractString)\n\nFunction to tokenize given text with tokenizer bpe encoder (t.bpe_encode). Returns a string vector of tokens.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.encode","page":"API","title":"PPLM.encode","text":"encode(t::GPT2Tokenizer, text::AbstractString; add_prefix_space=false)\n\nReturns the encoded vector of tokens (mapping from vocab of Tokenizer) for text. If add_prefix_space=true, add space at the start of 'text' before tokenization. \n\nExample\n\nFor single text:\n\nencode(tokenizer, text)\n\nFor vector of text:\n\nmap(x->encode(tokenizer, x), text_vector) \n\n\n\n\n\nencode(t::GPT2Tokenizer, tokens::Vector{String})\n\nFunction to encode tokens vectors to their integer mapping from vocab of tokenizer.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.decode","page":"API","title":"PPLM.decode","text":"decode(vocab::Vocabulary{T}, is::Vector{Int}) where T\n\nReturn decoded vector of string tokens from the indices vector is, using the vocab.\n\n\n\n\n\ndecode(t::GPT2Tokenizer, tokens_ids::Vector{Int})\n\nReturn decoded vector of string tokens from the indices vector tokens_ids, using the tokenizer t encoder .\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.detokenize","page":"API","title":"PPLM.detokenize","text":"detokenize(t::GPT2Tokenizer, tokens::Vector{String})\n\nBPE Decode the vector of strings, using the tokenizer t.\n\n\n\n\n\ndetokenize(t::GPT2Tokenizer, tokens_ids::Vector{Int})\n\nDecode and Detokenize the vector of indices token_ids. Returns the final sentence after detokenization.\n\nExample\n\nFor single vector of token_ids:\n\ndetokenize(tokenizer, token_ids)\n\nFor vector of vector of token_ids, use:\n\nmap(x->decode(tokenizer, x), tokens_id_vector_of_vector)\n\n\n\n\n\n","category":"function"},{"location":"api/#Discriminator-Model","page":"API","title":"Discriminator Model","text":"","category":"section"},{"location":"api/#General","page":"API","title":"General","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"PPLM.ClassifierHead\nPPLM.get_discriminator\nPPLM.save_classifier_head\nPPLM.save_discriminator","category":"page"},{"location":"api/#PPLM.ClassifierHead","page":"API","title":"PPLM.ClassifierHead","text":"struct ClassifierHead     linearlayer::Dense     embedsize::Int     class_size::Int end\n\nStruct for ClassifiedHead, defined with a single linear layer and two paramters: embedsize-> Size of Embedding, classsize->Number of classes.\n\n\n\n\n\n","category":"type"},{"location":"api/#PPLM.get_discriminator","page":"API","title":"PPLM.get_discriminator","text":"get_discriminator(model; load_from_pretrained=false, discrim=nothing, file_name=nothing, version=2, class_size::Int=1, embed_size::Int=768, path=nothing)\n\nFunction to create discriminator based on provided model. Incase, load_from_pretrained is set to be true, loads ClassifierHead layer from pretrained models or path provided.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.save_classifier_head","page":"API","title":"PPLM.save_classifier_head","text":"save_classifier_head(cl_head; file_name=nothing, path=nothing, args=nothing, register_discrim=true, discrim_name=\"\")\n\nFunction to save the ClassifiedHead as a BSON once the training is complete, based on the path provided. In case path is set as nothing, it saves the discriminators in ./pretrained_discriminators folder relative to the package directory.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.save_discriminator","page":"API","title":"PPLM.save_discriminator","text":"save_discriminator(discrim, discrim_name=\"Custom\"; file_name=nothing, path=nothing, args=nothing)\n\nFunction to save ClassifiedHead part of discriminator (by calling save_classifier_head function), which is the only trainable part of discriminator\n\n\n\n\n\n","category":"function"},{"location":"api/#Data-Processing","page":"API","title":"Data Processing","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"PPLM.pad_seq\nPPLM.get_mask\nPPLM.data_preprocess\nPPLM.load_data\nPPLM.load_cached_data\nPPLM.load_data_from_csv","category":"page"},{"location":"api/#PPLM.pad_seq","page":"API","title":"PPLM.pad_seq","text":"pad_seq(batch::AbstractVector{T}, pad_token::Integer=0)\n\nFunction to add pad tokens in shorter sequence, to make the length of each sequence equal to the max_length ( calculated as max(map(length, batch))) in the batch. Pad token defaults to 0. \n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.get_mask","page":"API","title":"PPLM.get_mask","text":"get_mask(seq::AbstractMatrix{T}, pad_token::Integer=0, embed_size::Integer=768)\n\nFunction to create mask for sequences against padding, so as to inform the model, that some part of sequenece is padded and hence to be ignored.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.data_preprocess","page":"API","title":"PPLM.data_preprocess","text":"data_preprocess(data_x, data_y, classification_type::String=\"Binary\", num_classes::Integer=2; args=nothing)\n\nFunction to preprocess data_x and data_y along with creating mask for the data_x. \n\nPreprocessing for data_x consist of padding with pad token (expected to be provided as args.pad_token).\n\nPreprocessing for data_y consist of creating onehotbach for data_y (if classification_type is not \"Binary\"), for 1:num_classes else reshape the data as (1, length(data_y)) \n\nReturns data_x, data_y, mask after pre-processing.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.load_data","page":"API","title":"PPLM.load_data","text":"load_data(data_x, data_y, tokenizer::PretrainedTokenizer;  batchsize::Integer=8, truncate::Bool=false, max_length::Integer=256, shuffle::Bool=false, drop_last::Bool=false, add_eos_start::Bool=true)\n\nReturns DataLoader for the data_x and data_y after processing the datax, with batchsize=batchsize. The processing consist of tokenization of datax and further truncation to max_len if truncate is set to be true. \n\nIf add_eos_start is set to true, add EOS token of tokenizer to the start. \n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.load_cached_data","page":"API","title":"PPLM.load_cached_data","text":"load_cached_data(discrim::Union{DiscriminatorV1, DiscriminatorV2}, data_x, data_y, tokenizer::PretrainedTokenizer; truncate::Bool=false, max_length::Integer=256, shuffle::Bool=false, batchsize::Int=4, drop_last::Bool=false, classification_type=\"Binary\", num_classes=2, args=nothing)\n\nReturns a DataLoader with (x, y) which can directly be feeded into classifier layer for training. \n\nThe function first loads the data using load_data function with batchsize=1, then passes each batch to the transformer model of discrim after data preprocessing, and then the average representation of the hidden_states are stored in a vector, which are then further loaded into a DataLoader, ready to use for classification training. \n\nNote: This functions saves time by cacheing the average representation of hidden states beforehand, avoiding passing the data through model in each epoch of training. This can be done as the model itself is non-trainable while training discriminator classifier head.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.load_data_from_csv","page":"API","title":"PPLM.load_data_from_csv","text":"load_data_from_csv(path_to_csv; text_col=\"text\", label_col=\"label\", delim=',', header=1)\n\nLoad the data from a csv file based on the specified text_col column for text and label_col for target label. Returns vectors for text and label.\n\n\n\n\n\n","category":"function"},{"location":"api/#Training","page":"API","title":"Training","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"PPLM.train!\nPPLM.test!\nPPLM.train_discriminator","category":"page"},{"location":"api/#PPLM.train!","page":"API","title":"PPLM.train!","text":"train!(discrim, data_loader; args=args)\n\nTrain the discriminator using the provided data_loader training data and arguments args provided.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.test!","page":"API","title":"PPLM.test!","text":"test!(discrim, data_loader; args=nothing)\n\nTest the discriminator on test data provided using data_loader, based on Accuracy and NLL Loss. \n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.train_discriminator","page":"API","title":"PPLM.train_discriminator","text":"train_discriminator(text, labels, batchsize::Int=8, classification_type::String=\"Binary\", num_classes::Int=2; model=\"gpt2\", cached::Bool=true, discrim=nothing, tokenizer=nothing, truncate=true, max_length=256, train_size::Float64=0.9, lr::Float64=1e-5, epochs::Int=10, args=nothing)\n\nFunction to train discriminator for provided text and target labels, based on set of function paramters provided. Returns discrim discriminator after training.\n\nHere the cached=true allows cacheing of contexualized embeddings (forward pass) in GPT2 model, as the model itself is non-trainable. This reduces the time of training effectively as the forward pass through GPT2 model is to be done only once.\n\nExample\n\nConsider a Multiclass classification problem with class size of 5, it can trained on text and labels vectors using:\n\ntrain_discriminator(text, labels, 16, \"Multiclass\", 5)\n\n\n\n\n\n","category":"function"},{"location":"api/#Bag-of-Words","page":"API","title":"Bag of Words","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"PPLM.get_bow_indices\nPPLM.build_bow_ohe","category":"page"},{"location":"api/#PPLM.get_bow_indices","page":"API","title":"PPLM.get_bow_indices","text":"get_bow_indices(bow_key_or_path_list::Vector{String}, tokenizer)\n\nReturns a list of list of indices of words from each Bag of word in the bow_key_or_path_list, after tokenization. The functions looks for provided BoW key in the registered artifacts Artifacts.toml file. In case not present there, function expects that bow_key is provided as the complete path to the file the URL to download .txt file.\n\nExample\n\nget_bow_indices([\"legal\", \"military\"])\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.build_bow_ohe","page":"API","title":"PPLM.build_bow_ohe","text":"build_bow_ohe(bow_indices, tokenizer)\n\nBuild and return a list of one_hot_matrix for each Bag Of Words list from indices. Each item of the list is of dimension (num_of_BoW_words, tokenizer.vocab_size). \n\nNote: While building the OHE of word indices, it only keeps those words, which have length 1 after tokenization and discard the rest.\n\n\n\n\n\n","category":"function"},{"location":"api/#Generation","page":"API","title":"Generation","text":"","category":"section"},{"location":"api/#Normal","page":"API","title":"Normal","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"PPLM.sample_normal","category":"page"},{"location":"api/#PPLM.sample_normal","page":"API","title":"PPLM.sample_normal","text":"sample_normal(;prompt=\"I hate the customs\", tokenizer=nothing, model=nothing, max_length=100, method=\"top_k\", k=50, t=1.2, p=0.5, add_eos_start=true)\n\nFunction to generate normal Sentences with model and tokenizer provided. In case not provided, function itself create instance of GPT2-small tokenizer and LM Head Model. The sentences are started with the provided prompt, and generated till token length reaches max_length.\n\nTwo sampling methods of generation are provided with this function:\n\nmethod='top_k'\nmethod='nucleus'\n\nAny of these methods can be used provided with either k or p.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM","page":"API","title":"PPLM","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"PPLM.sample_pplm\nPPLM.perturb_probs\nPPLM.perturb_hidden_bow\nPPLM.perturb_past_bow\nPPLM.perturb_hidden_discrim\nPPLM.perturb_past_discrim","category":"page"},{"location":"api/#PPLM.sample_pplm","page":"API","title":"PPLM.sample_pplm","text":"function sample_pplm(pplm_args;tokenizer=nothing, model=nothing, prompt=\"I hate the customs\", sample_method=\"top_k\", add_eos_start=true)\n\nFunction for PPLM model based generation. Generate perturbed sentence using pplm_args, tokenizer and model (GPT2, in case not provided), starting with prompt. In this function the generation is based on the arguments/parameters provided in pplm_args, which is an instance of pplm struct.  \n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.perturb_probs","page":"API","title":"PPLM.perturb_probs","text":"perturb_probs(probs, tokenizer, args)\n\nPerturb probabilities probs based on provided Bag of Words list (as given with args). This function is supported only for BoW model. \n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.perturb_hidden_bow","page":"API","title":"PPLM.perturb_hidden_bow","text":"perturb_hidden_bow(hidden, model, tokenizer, args)\n\nPerturb hidden states hidden based on provided Bag of Words list (as given with args). The perturbation is primarily based on the gradient calculated over losses evaluated over desired Bag of Words and KL Divergence from original token. \n\nAlso checkout perturb_hidden_discrim\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.perturb_past_bow","page":"API","title":"PPLM.perturb_past_bow","text":"perturb_past_bow(model, prev, past, original_probs, args)\n\nPerturb past key values prev based on provided Bag of Words list (as given with args). The perturbation is primarily based on the gradient calculated over losses evaluated over desired Bag of Words and KL Divergence from original token. \n\nAlso checkout perturb_past_discrim\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.perturb_hidden_discrim","page":"API","title":"PPLM.perturb_hidden_discrim","text":"perturb_hidden_discrim(hidden, model, tokenizer, args)\n\nPerturb hidden states hidden based on provided Discriminator (as given with args). The perturbation is primarily based on the gradient calculated over losses evaluated over desired Discriminator attribute and KL Divergence from original token. \n\nAlso checkout perturb_hidden_bow\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.perturb_past_discrim","page":"API","title":"PPLM.perturb_past_discrim","text":"perturb_past_discrim(model, prev, past, original_probs, args)\n\nPerturb past key values prev based on provided Discriminator (as given with args). The perturbation is primarily based on the gradient calculated over losses evaluated over desired Discriminator attribute and KL Divergence from original token.\n\nAlso checkout perturb_past_bow\n\n\n\n\n\n","category":"function"},{"location":"api/#Utils","page":"API","title":"Utils","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"PPLM.get_gpt2\nPPLM.get_gpt2_medium\nPPLM.set_device\nPPLM.get_registered_file\nPPLM.get_artifact\nPPLM.register_custom_file\nPPLM.top_k_sample\nPPLM.nucleus_sample\nPPLM.binary_accuracy\nPPLM.categorical_accuracy","category":"page"},{"location":"api/#PPLM.get_gpt2","page":"API","title":"PPLM.get_gpt2","text":"get_gpt2()\n\nFunction to load gpt2 lmheadmodel along with the tokenizer.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.get_gpt2_medium","page":"API","title":"PPLM.get_gpt2_medium","text":"get_gpt2_medium()\n\nFunction to load gpt2-medium lmhead model along with the tokenizer.\n\nNote: In case this function gives error of permission denied, try changing the file permissions for the Artifacts.toml file of Transformers.jl package (as it is read only by default) under the src/huggingface folder. \n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.set_device","page":"API","title":"PPLM.set_device","text":"set_device(d_id=0)\n\nFunction to set cuda device if available and also to disallow scalar operations\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.get_registered_file","page":"API","title":"PPLM.get_registered_file","text":"get_registered_file(name)\n\nFetch registered file path from Artifacts.toml, based on the artifact name.  \n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.get_artifact","page":"API","title":"PPLM.get_artifact","text":"get_artifact(name)\n\nUtility function to download/install the artifact in case not already installed.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.register_custom_file","page":"API","title":"PPLM.register_custom_file","text":"register_custom_file(artifact_name, file_name, path)\n\nFunction to register custom file under artifact_name in Artifacts.toml. path expects path of the directory where the file file_name is stored. Stores the complete path to the file as Artifact URL.\n\nExample\n\nregister_custom_file('custom', 'xyz.txt','./folder/folder/')\n\n\nNote: In case this gives permission denied error, change the Artifacts.toml file permissions using  chmod(path_to_file_in_julia_installation , 0o764)or similar.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.top_k_sample","page":"API","title":"PPLM.top_k_sample","text":"top_k_sample(probs; k=10)\n\nSampling function to return index from top_k probabilities, based on provided k. Function removes all tokens with a probability less than the last token of the top_k before sampling.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.nucleus_sample","page":"API","title":"PPLM.nucleus_sample","text":"nucleus_sample(probs; p=0.8)\n\nNuclues sampling function, to return after sampling reverse sorted probabilities probs till the index, where cumulative probability remains less than provided p. It removes tokens with cumulative probability above the threshold p before sampling.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.binary_accuracy","page":"API","title":"PPLM.binary_accuracy","text":"binary_accuracy(y_pred, y_true; threshold=0.5)\n\nCalculates Averaged Binary Accuracy based on y_pred and y_true. Argument threshold is used to specify the minimum predicted probability y_pred required to be labelled as 1. Default value set as 0.5.\n\n\n\n\n\n","category":"function"},{"location":"api/#PPLM.categorical_accuracy","page":"API","title":"PPLM.categorical_accuracy","text":"categorical_accuracy(y_pred, y_true)\n\nCalculates Averaged Categorical Accuracy based on y_pred and y_true.\n\n\n\n\n\n","category":"function"},{"location":"gpt2/#GPT2:-Tokenization-and-Generation","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"","category":"section"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"PPLM.jl comes along with support of GPT2 tokenizer and Generation. Here is an example of how you can use this feature of PPLM.jl package for tokenization and normal generation","category":"page"},{"location":"gpt2/#Tokenization","page":"GPT2: Tokenization and Generation","title":"Tokenization","text":"","category":"section"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"PPLM.jl allows users to load pre-trained GPT2 tokenizers based on BytePairEncoding.jl and Transformers.jl, which can then be used to tokenize/encode English text with a single line of code. The tokenizer is implemented with the following structure:","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"abstract type GPT2 <: PretrainedTokenizer end\n\nstruct GPT2Tokenizer <: GPT2\n    encoder::Vocabulary{String}\n    bpe_encode::GenericBPE\n    bpe_decode::UnMap\n    vocab::Dict{String, Any}\n    unk_token::String\n    unk_id::Int   \n    eos_token::String\n    eos_token_id::Int\n    pad_token::String\n    pad_token_id::Int\nend","category":"page"},{"location":"gpt2/#Example-of-Tokenization","page":"GPT2: Tokenization and Generation","title":"Example of Tokenization","text":"","category":"section"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"Let's see how you can tokenize text with PPLM.","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"# Load Tokenizer\nusing PPLM\ntokenizer = PPLM.load_pretrained_tokenizer(GPT2)\n\nsentence = \"This is an example of Tokenization\"","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"Once, you have loaded your tokenizer, one can use either of the methods:","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"tokens = tokenizer(sentence)\n# or \ntokens = encode(tokenizer, sentence)","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"It will return the following output:","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"7-element Vector{Int64}:\n  1213\n   319\n   282\n  1673\n   287\n 29131\n  1635","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"Now you have your list of tokens. Suppose you want to get back your sentence. This can be done in two ways:","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"# Firsth Method:\nsentence = detokenize(tokenizer, tokens)\n\n# Second Method:\ndecoded_tokens_list = decode(tokenizer, tokens)\t\n# returns vector: [\"This\", \"Ġis\", \"Ġan\", \"Ġexample\", \"Ġof\", \"ĠToken\", \"ization\"]\nsentence = detokenizer(tokenizer, decoded_tokens_list) ","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"You will get back your original sentence This is an example of Tokenization","category":"page"},{"location":"gpt2/#Generation-:-Normal-Text","page":"GPT2: Tokenization and Generation","title":"Generation : Normal Text","text":"","category":"section"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"PPLM.jl can be used to generate normal (unperturbed) text with the GPT2 model, with any of the two sampling methods top_k and nucleus:","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"To generate text, you can use the following code:","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"Here is a Sample text generated with GPT2 using the above code:","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"With Top_k sampling, k=50, prompt = \"Fruits are\"","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"\"Fruits are the key ingredient in our diet; their vitamins, and proteins are essential to build our immune system. \nWhat makes a good fruit one of them is simply as simple as your diet is. Fruit is one simple nutrient that is used \neffectively as a defense against sickness and stress (which can be very life changing indeed). When the body has \njust consumed enough fat for at least 40-50 days, the body also releases hormones known as the hormone estrogen in \norder to prevent infection. A good diet makes life easier\"\n","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"With Nucleus sampling, p=0.6, prompt = \"Fruits are\"","category":"page"},{"location":"gpt2/","page":"GPT2: Tokenization and Generation","title":"GPT2: Tokenization and Generation","text":"\"Fruits are packed with the goodness of ancient Greek life, plants that protect and revive us from death. At every \nstone in the garden, your fruit may reflect on the people who once carried you from town to town, those who would \nstill give you food to live, and the perfect pair of hand-gloved fingers you may wear in your golden bedroll.\"\n","category":"page"},{"location":"bow/#Bag-Of-Words-Model","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"","category":"section"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"In the BoW model, we have a list of words that correspond to that particular attribute, usually used in sentences that can be considered to have that attribute. For example, consider the case of Topic-based attribute, let's say Politics, where we want to drive the topic of generation towards politics, then a typical BagOfWord for Politics will include words like government, politics, democracy, federation, etc, etc. These words will then be used for perturbation through PPLM.jl.","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"Let's take up an example to understand how it works.","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"Let's first initialize the package and model.","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"using PPLM\n\ntokenizer, model = PPLM.get_gpt2();\nmodel = model |> PPLM.gpu","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"Prompt: To conclude","category":"page"},{"location":"bow/#Perturb-Probability","page":"Bag Of Words Model","title":"Perturb Probability","text":"","category":"section"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"This feature only supports for Bag of Words Model. Perturbation of probability can be done similar to the given example:","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"args= PPLM.pplm(perturb=\"probs\", bow_list = [\"politics\"], stepsize=1.0, fusion_gm_scale=0.8f0, top_k=50)\n\nPPLM.sample_pplm(args; tokenizer=tokenizer, model=model, prompt=\"To conclude\")\n","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"Another more crude way of generation could be:","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"input_ = [tokenizer.eos_token_id; tokenizer(\"To conclude\")]\n\nargs= PPLM.pplm(perturb=\"probs\", bow_list = [\"politics\"], stepsize=1.0, fusion_gm_scale=0.8f0, top_k=50)\n\nfor i in 1:100\n    input_ids = reshape(input_[:], :, 1)\n    outputs = model(input_ids; output_attentions=false,\n                        output_hidden_states=true,\n                        use_cache=false);\n    original_logits = outputs.logits[:, end, 1]\n    original_probs = PPLM.temp_softmax(original_logits; t=args.temperature)\n    pert_probs = PPLM.perturb_probs(original_probs, tokenizer, args)\n    gm_scale = args.fusion_gm_scale\n    pert_probs = Float32.((original_probs.^(1-gm_scale)).*(pert_probs.^(gm_scale)))\n    new_token = PPLM.top_k_sample(pert_probs; k=args.top_k)[1]\n    push!(input_, new_token)\nend\n\ntext = detokenize(tokenizer, input_)\n","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"Sample generation:","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"\"To conclude the last week about our current policy, they say we \\\"don't follow their dictates.\\\"[25][26] \nWe've never followed that precept, so in order, what we have is a very limited, almost not always followed \nagenda. It took decades to implement and it has already occurred once.[27] These are the arguments that \nmost conservative leaders used to put forth to show the world that our public spending has failed – as \nconservatives pointed out. What they don't seem to understand is that this ideology\"","category":"page"},{"location":"bow/#Perturb-Hidden-States","page":"Bag Of Words Model","title":"Perturb Hidden States","text":"","category":"section"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"Perturbation of hidden states can be done similar to the given example","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"args= PPLM.pplm(perturb=\"hidden\", bow_list = [\"politics\"], stepsize=0.02, fusion_gm_scale=0.8f0, top_k=50)\n\nPPLM.sample_pplm(args; tokenizer=tokenizer, model=model, prompt=\"To conclude\")\n","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"Another more crude way of generation could be:","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"\ninput_ = [tokenizer.eos_token_id; tokenizer(\"To conclude\")]\n\nargs= PPLM.pplm(perturb=\"hidden\", bow_list = [\"politics\"], stepsize=0.02, fusion_gm_scale=0.8f0, top_k=50)\n\nfor i in 1:100\n    input_ids = reshape(input_[:], :, 1) |> PPLM.gpu\n    outputs = model(input_ids; output_attentions=false,\n                        output_hidden_states=true,\n                        use_cache=false);\n    original_logits = outputs.logits[:, end, 1]\n    original_probs = PPLM.temp_softmax(original_logits; t=args.temperature)\n    \n    hidden = outputs.hidden_states[end]\n    \n    modified_hidden = PPLM.perturb_hidden_bow(model, hidden, args)\n    pert_logits = model.lm_head(modified_hidden)[:, end, 1]\n    pert_probs = PPLM.temp_softmax(pert_logits; t=args.temperature)\n    \n    gm_scale = args.fusion_gm_scale\n    pert_probs = Float32.((original_probs.^(1-gm_scale)).*(pert_probs.^(gm_scale))) |> cpu\n    new_token = PPLM.top_k_sample(pert_probs; k=args.top_k)[1]\n    push!(input_, new_token)\n    #print(\".\")\nend\n\ntext = detokenize(tokenizer, input_)\n","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"Sample generation:","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"\"To conclude this brief essay, I have briefly discussed one of my own writing's main points: How a great \nmany poor working people who were forced by the government to sell goods to high-end supermarkets to make \nends meet were put off purchasing goods at a time they wouldn't be able afford. That point of distinction \narises in every social democracy I identify as libertarian.\\n\\nA large number of people in this group \nsimply did not follow basic political norms, and in order not to lose faith that politics was in\"","category":"page"},{"location":"bow/#Perturb-Past","page":"Bag Of Words Model","title":"Perturb Past","text":"","category":"section"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"args= PPLM.pplm(perturb=\"past\", bow_list = [\"politics\"], stepsize=0.005, fusion_gm_scale=0.8f0, top_k=50)\n\nPPLM.sample_pplm(args; tokenizer=tokenizer, model=model, prompt=\"To conclude\")\n","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"Another more crude way of generation could be:","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"input_ = [tokenizer.eos_token_id; tokenizer(\"To conclude\")]\n\nargs= PPLM.pplm(perturb=\"past\", bow_list = [\"politics\"], stepsize=0.005, fusion_gm_scale=0.8f0, top_k=50)\n\nfor i in 1:100\n    input_ids = reshape(input_[:], :, 1) |> PPLM.gpu\n    inp = input_ids[1:end-1, :]\n    prev = input_ids[end:end, :]\n    outputs = model(inp; output_attentions=false,\n                        output_hidden_states=true,\n                        use_cache=true);\n    past = outputs.past_key_values;\n    original_logits = outputs.logits[:, end, 1]\n    original_probs = PPLM.temp_softmax(original_logits; t=args.temperature)\n    \n    new_past = PPLM.perturb_past_bow(model, prev, past, original_probs, args)\n    output_new = model(prev; past_key_values=new_past,\n                                        output_attentions=false,\n                                        output_hidden_states=true,\n                                        use_cache=true);    \n    pert_logits = output_new.logits[:, end, 1]\n    pert_probs = PPLM.temp_softmax(pert_logits; t=args.temperature)\n    \n    gm_scale = args.fusion_gm_scale\n    pert_probs = Float32.((original_probs.^(1-gm_scale)).*(pert_probs.^(gm_scale))) |> cpu\n    new_token = PPLM.top_k_sample(pert_probs; k=args.top_k)[1]\n    push!(input_, new_token)\n    #print(\".\")\nend\n\ntext = detokenize(tokenizer, input_)\n","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"Sample generation:","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"\"To conclude, it's important for governments, from the government of Canada, who decide matters of \ninternational importance and culture and language issues to the authorities the responsible party for \nimmigration enforcement, when that person's an international terrorist, as these are important and cultural \ncommunities, rather and international business people, like the Canadian government, should take seriously \nwhen they say these, to the authorities, and then have the Canadian people deal with, and to them be more \ninvolved in the process itself and their work ethics should really be to\"","category":"page"},{"location":"bow/","page":"Bag Of Words Model","title":"Bag Of Words Model","text":"Note: For different topics, you may need to tune some hyperparameters like stepsize, fusion_gm_scale etc. to get some really interesting results. Will add more details on it later. Also note that in first iteration, it usually takes more time to evaluate the gradients but becomes fast in consecutive passes.","category":"page"},{"location":"how/#How-does-PPLM-works?","page":"How does it work?","title":"How does PPLM works?","text":"","category":"section"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"Plug and Play Language Models or PPLM controls the generation of sentences in Large Language Models like GPT2 using gradient based steering towards control attributes. The gradient is evaluated based on Attribute Model being used, which is then used to perturb hidden state or past key values of the model. The perturbation is applied such that it pushes the generation towards the desired attribute.","category":"page"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"(Image: PPLM)","category":"page"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"Figure 1 Simplified version of how PPLM works","category":"page"},{"location":"how/#Attribute-Models","page":"How does it work?","title":"Attribute Models","text":"","category":"section"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"PPLM.jl provides with the following two Attribute models:","category":"page"},{"location":"how/#Bag-Of-Words-or-BoW-Model","page":"How does it work?","title":"Bag Of Words or BoW Model","text":"","category":"section"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"Bag of Words model consist of a list of words that belong to a particular Topic. The loss for this model is evaluated so as to increase probability of words belonging to this list while decreasing the rest, hence, pushing the genration towards desired topic.","category":"page"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"Bag of words available are: legal, military, politics, monsters, science, space, technology, religion and positive_words. Source: https://github.com/uber-research/PPLM","category":"page"},{"location":"how/#Discriminator-Model","page":"How does it work?","title":"Discriminator Model","text":"","category":"section"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"Discriminator Model consist of a trainable ClassifierHead which is basically a Linear layer. The layer is trained for classification of desired attribute. This trained model is then further used to calculate the gradients based on crossentropy loss of generation.","category":"page"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"PPLM.jl currently supports Detoxification model for GPT2-small and sentiment and clickbait for GPT2-medium. (Note: GPT2-medium may give some errors while downloading the model files, because of some file mode issue in Transformers.jl. To rectify, please change the mode of Transformers.jl Huggingface Artifacts.toml file, to allow loading the GPT2 model).","category":"page"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"PPLM.jl further supports training your own Discriminator model and then generate with that model, by providing path to the saved BSON file of model ClassifierHead, along with config_metadata. ","category":"page"},{"location":"how/#Support","page":"How does it work?","title":"Support","text":"","category":"section"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"PPLM.jl provides support for attribute control based on:","category":"page"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"Hidden State Perturbation\nPast Key Values Perturbation","category":"page"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"For more details, check out the implementation in the repo.","category":"page"},{"location":"how/#PPLM","page":"How does it work?","title":"PPLM","text":"","category":"section"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"PPLM struct for hyperparameters for generation looks like this:","category":"page"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"@with_kw struct pplm\n    method::String=\"BoW\"\n    perturb::String=\"hidden\"     # hidden or past -> hidden support BoW only without gradient based change\n    bow_list::Vector{String}=[\"military\"]\n    discrim::String=\"toxicity\"\n    embed_size::Int=768\n    target_class_id=-1\n    file_name::String=\"\"\n    path::String=\"\"\n    stepsize::Float32=0.01      \n    max_length::Int=100\n    num_iterations::Int=2        # more the number of iterations, more updates, more time to update\n    top_k::Int=50\n    top_p::Float32=0.8\n    temperature::Float32=1.1\n    fusion_gm_scale::Float32=0.9\n    fusion_kl_scale::Float32=0.01\n    cuda::Tuple{Bool, Int64}=(CUDA.has_cuda(), device_id)\n    window_length::Int=0          # window length 0 corresponds to infinite length\n    gamma::Float32=1.5\nend","category":"page"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"These hyperparameters can be tuned as per the desired attribute. Some of the examples of pplm arguments looks like this:","category":"page"},{"location":"how/","page":"How does it work?","title":"How does it work?","text":"# bow model\nargs = pplm(method=\"BoW\", bow_list=[\"legal\"], num_iterations=3)\n\n# discriminator model\nargs = pplm(method=\"Discrim\", perturb=\"past\", stepsize=0.02)","category":"page"},{"location":"discrim/#Discriminator-Model","page":"Discriminator Model","title":"Discriminator Model","text":"","category":"section"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"In the BoW Model, we first train a Linear Discriminator over the Large Language Model called as ClassifierHead, to classify wanted vs unwanted class. This ClassifierHead is then used to calculate gradients against the crossentropy loss for p(a/x) where a is the desired attribute. ","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"In PPLM.jl, the ClassifierHead is defined as a struct:","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"struct ClassifierHead\n    linear_layer::Dense\n    embed_size::Int\n    class_size::Int\nend","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"You can load a ClassifierHead with any of the following Methods:","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"#Method 1: load a pretrained model\nclassifier, config_metadata = PPLM.ClassifierHead(;load_from_pretrained=true, discrim=\"toxicity\")    \n\n#Method 2: load a custom trained mode\nclassifier, config_metadata = PPLM.ClassifierHead(;load_from_pretrained=true, path=\"./pretrained/custom_model.bson\") \n\n#Method 3: Intiate a random Classifier Layer\nclassifier, _ = PPLM.ClassifierHead(;load_from_pretrained=true, discrim=\"toxicity\") \n","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"Let's delve into an example of PPLM based generation with Discriminator Model.","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"First, let's load the package and model:","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"using PPLM\n\ntokenizer, model = PPLM.get_gpt2();","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"Prompt: Do I look like I give a","category":"page"},{"location":"discrim/#Perturb-Hidden-State","page":"Discriminator Model","title":"Perturb Hidden State","text":"","category":"section"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"Perturbation of hidden states can be done similar to the given example","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"args = PPLM.pplm(method=\"Discrim\", perturb=\"hidden\", discrim=\"toxicity\", target_class_id=1, stepsize=0.008, fusion_kl_scale=0.05);\n\nPPLM.sample_pplm(args; tokenizer=tokenizer, model=model, prompt=\"Do I look like I give a\")\n","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"Another more crude way of generation could be:","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"\ninput_ = [tokenizer.eos_token_id; tokenizer(\"Do I look like I give a\")]\n\nargs = PPLM.pplm(method=\"Discrim\", perturb=\"hidden\", discrim=\"toxicity\", target_class_id=1, stepsize=0.008, fusion_kl_scale=0.05);\n\nfor i in 1:100\n    input_ids = reshape(input_[:], :, 1) |> PPLM.gpu\n    outputs = model(input_ids; output_attentions=false,\n                        output_hidden_states=true,\n                        use_cache=false);\n    original_logits = outputs.logits[:, end, 1]\n    original_probs = PPLM.temp_softmax(original_logits; t=args.temperature)\n    \n    hidden = outputs.hidden_states[end]\n    modified_hidden = PPLM.perturb_hidden_discrim(hidden, model, tokenizer, args)\n    pert_logits = model.lm_head(modified_hidden)[:, end, 1]\n    pert_probs = PPLM.temp_softmax(pert_logits; t=args.temperature)\n    \n    gm_scale = args.fusion_gm_scale\n    pert_probs = Float32.((original_probs.^(1-gm_scale)).*(pert_probs.^(gm_scale))) |> cpu\n    new_token = PPLM.top_k_sample(pert_probs; k=args.top_k)[1]\n    push!(input_, new_token)\nend\n\ntext = detokenize(tokenizer, input_)\n","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"Sample generation:","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"\"Do I look like I give a damn? I want to be a nice person who treats my colleagues and even friends \nlike people.\\n\\nFor one thing, it takes time for me and others to really consider and think about \nyour value. In the past, I often felt uncomfortable working with people who thought my interests, \nopinions and interests were different, and didn't have the emotional and spiritual value to interact \nwith them. I didn't feel like they wanted me to speak to their views. So I started getting involved \non many other topics\"","category":"page"},{"location":"discrim/#Perturb-Past-Key-Values","page":"Discriminator Model","title":"Perturb Past Key Values","text":"","category":"section"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"Perturbation of hidden states can be done similar to the given example","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"args = PPLM.pplm(method=\"Discrim\", perturb=\"past\", discrim=\"toxicity\", target_class_id=1, stepsize=0.004, fusion_kl_scale=0.05);\n\nPPLM.sample_pplm(args; tokenizer=tokenizer, model=model, prompt=\"Do I look like I give a\")\n","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"Another more crude way of generation could be:","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"\ninput_ = [tokenizer.eos_token_id; tokenizer(\"Do I look like I give a\")]\n\nargs = PPLM.pplm(method=\"Discrim\", perturb=\"past\", discrim=\"toxicity\", target_class_id=1, stepsize=0.008, fusion_kl_scale=0.05);\n\nfor i in 1:100\n    input_ids = reshape(input_[:], :, 1) |> PPLM.gpu\n    inp = input_ids[1:end-1, :]\n    prev = input_ids[end:end, :]\n    outputs = model(inp; output_attentions=false,\n                        output_hidden_states=true,\n                        use_cache=true);\n    past = outputs.past_key_values;\n    original_logits = outputs.logits[:, end, 1]\n    original_probs = PPLM.temp_softmax(original_logits; t=args.temperature)\n    \n    new_past = PPLM.perturb_past_discrim(model, prev, past, original_probs, args)\n    output_new = model(prev; past_key_values=new_past,\n                                        output_attentions=false,\n                                        output_hidden_states=true,\n                                        use_cache=true);    \n    pert_logits = output_new.logits[:, end, 1]\n    pert_probs = PPLM.temp_softmax(pert_logits; t=args.temperature)\n    #print(sum(pert_probs.-original_probs))\n    \n    gm_scale = args.fusion_gm_scale\n    pert_probs = Float32.((original_probs.^(1-gm_scale)).*(pert_probs.^(gm_scale))) |> cpu\n    new_token = PPLM.top_k_sample(pert_probs; k=args.top_k)[1]\n    push!(input_, new_token)\nend\n\ntext = detokenize(tokenizer, input_)\n","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"Sample generation:","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"\"Do I look like I give a proper treatment to these people? We're seeing real examples in all the \nthings that they have done as well. There is going to be a discussion on there with the state of \nwhat steps we should be taking to address all cases of people in the community, and then what we \nare going to do going forward that has not a national interest interest. Is your experience with \nsimilar issues from different different sides affected your work/responsibility of not doing that \nthings you find seem quite simple, at first glance?\"","category":"page"},{"location":"discrim/#Load-Custom-Model","page":"Discriminator Model","title":"Load Custom Model","text":"","category":"section"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"You can use your own custom train Model (suppose saved at path=path) using the following:","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"args = PPLM.pplm(method=\"Discrim\", discrim=\"custom\", path=path, target_class_id=1, stepsize=0.008, fusion_kl_scale=0.05);\n\nPPLM.sample_pplm(args; tokenizer=tokenizer, model=model, prompt=\"Do I look like I give a\")\n","category":"page"},{"location":"discrim/","page":"Discriminator Model","title":"Discriminator Model","text":"Note: For different Discriminator, you may need to tune some hyperparameters like stepsize, fusion_gm_scale etc. to get some really interesting results. Will add more details on it later. Also note that in first iteration, it usually takes more time to evaluate the gradients but becomes fast in consecutive passes.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = PPLM","category":"page"},{"location":"#PPLM.jl","page":"Home","title":"PPLM.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"PPLM.jl is a julia based implementation of Plug and Play Language Models. The implementation is primarily based on Transformers.jl GPT2 and allows user to steer the Text generation task based on some Attribute Models.","category":"page"},{"location":"#Why-PPLM.jl?","page":"Home","title":"Why PPLM.jl?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"While large pretrained language models can generate coherent text, it's hard to control the context are actually generating.  Plug and Play Language Models or PPLM allows a user to flexibly plug in one or more tiny attribute models representing the desired steering objective into a large, unconditional language model (LM). While the main feature of this package is to help with Controlled Text generation, it also facilitates the following through simple API functions: ","category":"page"},{"location":"","page":"Home","title":"Home","text":"GPT2 pretrained Tokenizer\nNormal Text generation with GPT2 using few lines of code.\nPretrained Discriminators from Huggingface loaded as BSON file. \nSome predefined BagofWords.\nDiscriminator Training -  Linear layer classifier on GPT2 model\nSome more options for Controlled generation of Text, as an extension to PPLM.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In order to install the PPLM package, write the following in the Julia Prompt:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add PPLM","category":"page"},{"location":"","page":"Home","title":"Home","text":"or","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"PPLM\")","category":"page"},{"location":"contact/#Contact-Info","page":"Contact Info","title":"Contact Info","text":"","category":"section"},{"location":"contact/","page":"Contact Info","title":"Contact Info","text":"In case you have any doubts regarding any part of the Package or it's implementation, feel free to create an issue in the repository or leave a message at adarshkumar712@gmail.com. Apart from these, if you feel some feature can be added to this repo, or if you want to contribute to this repository, again you can do as aforementioned.","category":"page"},{"location":"contact/","page":"Contact Info","title":"Contact Info","text":"Also you can mention @adarshkumar712 in slack #natural_language to discuss what so ever in your mind.","category":"page"},{"location":"discrim_train/#Discriminator-Training","page":"Discriminator Training","title":"Discriminator Training","text":"","category":"section"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"PPLM.jl allows it's used not allow to use pretrained Discriminators but also to train their custom discriminator. It provide functions ranging from Data Preprocessing, Caching to Training and the saving the discriminator.","category":"page"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"Let's understand how to use PPLM.jl to train your own custom discriminator. ","category":"page"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"Consider that you have a list of text and a corresponding list of labels (expected to range from 0 to n-1 where n is the number of classes) for which you want to train your discriminator on.","category":"page"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"First let's load the PPLM package and the model:","category":"page"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"using PPLM\n\ntokenizer, model = PPLM.get_gpt2();\n","category":"page"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"Once we have our model loaded, let's intialize the Hyperparameters required and the Discriminator:","category":"page"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"args = PPLM.HyperParams(lr=5e-6,classification_type=\"MultiClass\", epochs=50)\ndiscrim = PPLM.get_discriminator(model; class_size=2);","category":"page"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"Now you can use any of the following two Methods for training your own discriminator (usually even if its a binary problem, PPLM treat it as a Multiclass problem, same as in original repo of PPLM by Uber).","category":"page"},{"location":"discrim_train/#Method-1","page":"Discriminator Training","title":"Method 1","text":"","category":"section"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"PPLM.train_discriminator(text, labels, 8, \"Multiclass\", 2; lr=5e-6, discrim=discrim, tokenizer=tokenizer, args=args, train_size=0.85, epochs=50);\n\n# It will automatically create a test data split and evaluate the model on that data.","category":"page"},{"location":"discrim_train/#Method-2","page":"Discriminator Training","title":"Method 2","text":"","category":"section"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"using StatsBase\nusing Random\n\n(train_x, train_y), (test_x, test_y) = PPLM.splitobs((text_reduced, label_reduced); at=0.8);\n        \ntrain_loader = PPLM.load_cached_data(discrim, train_x, train_y, tokenizer; truncate=true, classification_type=\"Multiclass\");\n\ntest_loader = PPLM.load_cached_data(discrim, test_x, test_y, tokenizer; truncate=true, classification_type=\"Multiclass\");\n\nPPLM.train!(discrim, train_loader; args=args);\n\nPPLM.test!(discrim, test_loader; args=args)","category":"page"},{"location":"discrim_train/#Save-Discriminator","page":"Discriminator Training","title":"Save Discriminator","text":"","category":"section"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"Once you have trained your Discriminator, you can save it as follows:","category":"page"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"path = \"replace_it_with_the_path_you_want_to_the_directory\"\nPPLM.save_discriminator(discrim, \"custom_discriminator\"; file_name=\"custom_model.bson\", path= path)\n","category":"page"},{"location":"discrim_train/#Load-Discriminator","page":"Discriminator Training","title":"Load Discriminator","text":"","category":"section"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"To load the discriminator you saved, you can do as follows:","category":"page"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"\ntokenizer, model = PPLM.get_gpt2();\n\npath_file = joinpath(path, file_name) # path = path to the directory\n\ndiscrim = PPLM.get_discriminator(model; load_from_pretrained=true, discrim=\"custom\", path=path, class_size=2);\n","category":"page"},{"location":"discrim_train/","page":"Discriminator Training","title":"Discriminator Training","text":"For more details, you can check out the code in the repo.","category":"page"}]
}
