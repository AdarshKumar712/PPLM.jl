var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = PPLM","category":"page"},{"location":"#PPLM","page":"Home","title":"PPLM","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for PPLM.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [PPLM]","category":"page"},{"location":"#PPLM.GPT2Tokenizer","page":"Home","title":"PPLM.GPT2Tokenizer","text":"struct GPT2Tokenizer <: GPT2     encoder::Vocabulary{String}     bpeencode::GenericBPE     bpedecode::UnMap     vocab::Dict{String, Any}     unktoken::String     unkid::Int        eostoken::String     eostokenid::Int     padtoken::String     padtokenid::Int end\n\nStructure to hold all essential information / functions for GPT2 tokenizer\n\n\n\n\n\n","category":"type"},{"location":"#PPLM.GPT2Tokenizer-Tuple{AbstractString}","page":"Home","title":"PPLM.GPT2Tokenizer","text":"(t::GPT2Tokenizer)(text::AbstractString; add_prefix_space=false)\n\nEncode the text with tokenizer and returns the encoded vector. If add_prefix_space=true, add space at the start of 'text' before tokenization. \n\nExamples:\n\nFor a single text:\n\ntokenizer(text; add_prefix_space=true)\n\nFor vector of texts, use:\n\nmap(x->encode(tokenizer, x), text_vector) \n\n# or\n\ntokenizer.(text_vector)\n\nAlso checkout encode\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.build_bow_ohe-Tuple{Any, Any}","page":"Home","title":"PPLM.build_bow_ohe","text":"build_bow_ohe(bow_indices, tokenizer)\n\nBuild and return a list of one_hot_matrix for each Bag Of Words list from indices. Each item of the list is of dimension (num_of_BoW_words, tokenizer.vocab_size). \n\nNote: While building the OHE of word indices, it only keeps those words, which have length 1 after tokenization and discard the rest.\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.data_preprocess","page":"Home","title":"PPLM.data_preprocess","text":"data_preprocess(data_x, data_y, classification_type::String=\"Binary\", num_classes::Integer=2; args=nothing)\n\nFunction to preprocess data_x and data_y along with creating mask for the data_x. \n\nPreprocessing for data_x consist of padding with pad token (expected to be provided as args.pad_token).\n\nPreprocessing for data_y consist of creating onehotbach for data_y (if classification_type is not \"Binary\"), for 1:num_classes else reshape the data as (1, length(data_y)) \n\nReturns data_x, data_y, mask after pre-processing.\n\n\n\n\n\n","category":"function"},{"location":"#PPLM.decode-Tuple{PPLM.GPT2Tokenizer, Vector{Int64}}","page":"Home","title":"PPLM.decode","text":"decode(t::GPT2Tokenizer, tokens_ids::Vector{Int})\n\nReturn decoded vector of string tokens from the indices vector tokens_ids, using the tokenizer t encoder .\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.decode-Union{Tuple{T}, Tuple{Transformers.Basic.Vocabulary{T}, Vector{Int64}}} where T","page":"Home","title":"PPLM.decode","text":"decode(vocab::Vocabulary{T}, is::Vector{Int}) where T\n\nReturn decoded vector of string tokens from the indices vector is, using the vocab.\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.detokenize-Tuple{PPLM.GPT2Tokenizer, Vector{Int64}}","page":"Home","title":"PPLM.detokenize","text":"detokenize(t::GPT2Tokenizer, tokens_ids::Vector{Int})\n\nDecode and Detokenize the vector of indices token_ids. Returns the final sentence after detokenization.\n\nExample\n\nFor single vector of token_ids:\n\ndetokenize(tokenizer, token_ids)\n\nFor vector of vector of token_ids, use:\n\nmap(x->decode(tokenizer, x), tokens_id_vector_of_vector)\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.detokenize-Tuple{PPLM.GPT2Tokenizer, Vector{String}}","page":"Home","title":"PPLM.detokenize","text":"detokenize(t::GPT2Tokenizer, tokens::Vector{String})\n\nBPE Decode the vector of strings, using the tokenizer t.\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.encode-Tuple{PPLM.GPT2Tokenizer, AbstractString}","page":"Home","title":"PPLM.encode","text":"encode(t::GPT2Tokenizer, text::AbstractString; add_prefix_space=false)\n\nReturns the encoded vector of tokens (mapping from vocab of Tokenizer) for text. If add_prefix_space=true, add space at the start of 'text' before tokenization. \n\nExample\n\nFor single text:\n\nencode(tokenizer, text)\n\nFor vector of text:\n\nmap(x->encode(tokenizer, x), text_vector) \n\n\n\n\n\n","category":"method"},{"location":"#PPLM.encode-Tuple{PPLM.GPT2Tokenizer, Vector{String}}","page":"Home","title":"PPLM.encode","text":"encode(t::GPT2Tokenizer, tokens::Vector{String})\n\nFunction to encode tokens vectors to their integer mapping from vocab of tokenizer.\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.get_artifact-Tuple{Any}","page":"Home","title":"PPLM.get_artifact","text":"get_artifact(name)\n\nUtility function to download/install the artifact in case not already installed.\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.get_bow_indices-Tuple{Vector{String}, Any}","page":"Home","title":"PPLM.get_bow_indices","text":"get_bow_indices(bow_key_or_path_list::Vector{String}, tokenizer)\n\nReturns a list of list of indices of words from each Bag of word in the bow_key_or_path_list, after tokenization. The functions looks for provided BoW key in the registered artifacts Artifacts.toml file. In case not present there, function expects that bow_key is provided as the complete path to the file the URL to download .txt file.\n\nExample\n\nget_bow_indices([\"legal\", \"military\"])\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.get_mask-Union{Tuple{AbstractMatrix{T}}, Tuple{T}, Tuple{AbstractMatrix{T}, Integer}, Tuple{AbstractMatrix{T}, Integer, Integer}} where T","page":"Home","title":"PPLM.get_mask","text":"get_mask(seq::AbstractMatrix{T}, pad_token::Integer=0, embed_size::Integer=768)\n\nFunction to create mask for sequences against padding, so as to inform the model, that some part of sequenece is padded and hence to be ignored.\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.get_registered_file-Tuple{Any}","page":"Home","title":"PPLM.get_registered_file","text":"get_registered_file(name)\n\nFetch registered file path from Artifacts.toml, based on the artifact name.  \n\n\n\n\n\n","category":"method"},{"location":"#PPLM.load_cached_data-Tuple{Union{PPLM.DiscriminatorV1, DiscriminatorV2}, Any, Any, PPLM.PretrainedTokenizer}","page":"Home","title":"PPLM.load_cached_data","text":"load_cached_data(discrim::Union{DiscriminatorV1, DiscriminatorV2}, data_x, data_y, tokenizer::PretrainedTokenizer; truncate::Bool=false, max_length::Integer=256, shuffle::Bool=false, batchsize::Int=4, drop_last::Bool=false, classification_type=\"Binary\", num_classes=2, args=nothing)\n\nReturns a DataLoader with (x, y) which can directly be feeded into classifier layer for training. \n\nThe function first loads the data using load_data function with batchsize=1, then passes each batch to the transformer model of discrim after data preprocessing, and then the average representation of the hidden_states are stored in a vector, which are then further loaded into a DataLoader, ready to use for classification training. \n\nNote: This functions saves time by cacheing the average representation of hidden states beforehand, avoiding passing the data through model in each epoch of training. This can be done as the model itself is non-trainable while training discriminator classifier head.\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.load_data-Tuple{Any, Any, PPLM.PretrainedTokenizer}","page":"Home","title":"PPLM.load_data","text":"load_data(data_x, data_y, tokenizer::PretrainedTokenizer;  batchsize::Integer=8, truncate::Bool=false, max_length::Integer=256, shuffle::Bool=false, drop_last::Bool=false, add_eos_start::Bool=true)\n\nReturns DataLoader for the data_x and data_y after processing the datax, with batchsize=batchsize. The processing consist of tokenization of datax and further truncation to max_len if truncate is set to be true. \n\nIf add_eos_start is set to true, add EOS token of tokenizer to the start. \n\n\n\n\n\n","category":"method"},{"location":"#PPLM.load_data_from_csv-Tuple{Any}","page":"Home","title":"PPLM.load_data_from_csv","text":"load_data_from_csv(path_to_csv; text_col=\"text\", label_col=\"label\", delim=',', header=1)\n\nLoad the data from a csv file based on the specified text_col column for text and label_col for target label. Returns vectors for text and label.\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.load_pretrained_tokenizer-NTuple{5, Any}","page":"Home","title":"PPLM.load_pretrained_tokenizer","text":"load_pretrained_tokenizer(path_bpe, path_vocab, unk_token, eos_token, pad_token)\n\nLoad pretrained tokenizer for GPT2 from provided bpe and vocab file path. Initialises unk_token, eos_token, pad_token as provided with the function. Returns tokenizer as GPT2Tokenizer structure.\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.load_pretrained_tokenizer-Union{Tuple{Type{T}}, Tuple{T}} where T<:PPLM.PretrainedTokenizer","page":"Home","title":"PPLM.load_pretrained_tokenizer","text":"load_pretrained_tokenizer(ty::Type{T}; unk_token=\"<|endoftext|>\", eos_token=\"<|endoftext|>\", pad_token=\"<|endoftext|>\") where T<:PretrainedTokenizer\n\nLoad GPT2 tokenizer using Datadeps for pretrained bpe and vocab. Returns tokenizer as GPT2Tokenizer structure.\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.pad_seq-Union{Tuple{AbstractVector{T}}, Tuple{T}, Tuple{AbstractVector{T}, Integer}} where T","page":"Home","title":"PPLM.pad_seq","text":"pad_seq(batch::AbstractVector{T}, pad_token::Integer=0)\n\nFunction to add pad tokens in shorter sequence, to make the length of each sequence equal to the max_length ( calculated as max(map(length, batch))) in the batch. Pad token defaults to 0. \n\n\n\n\n\n","category":"method"},{"location":"#PPLM.register_custom_file-Tuple{Any, Any, Any}","page":"Home","title":"PPLM.register_custom_file","text":"register_custom_file(artifact_name, file_name, path)\n\nFunction to register custom file under artifact_name in Artifacts.toml. path expects path of the directory where the file file_name is stored. Stores the complete path to the file as Artifact URL.\n\nExample\n\nregister_custom_file('custom', 'xyz.txt','./folder/folder/')\n\n\nNote: In case this gives permission denied error, change the Artifacts.toml file permissions using  chmod(path_to_file_in_julia_installation , 0o764)or similar.\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.save_classifier_head-Tuple{Any}","page":"Home","title":"PPLM.save_classifier_head","text":"\n\n\n\n","category":"method"},{"location":"#PPLM.tokenize-Tuple{PPLM.GPT2Tokenizer, AbstractString}","page":"Home","title":"PPLM.tokenize","text":"tokenize(t::GPT2Tokenizer, text::AbstractString)\n\nFunction to tokenize given text with tokenizer bpe encoder (t.bpe_encode). Returns a string vector of tokens.\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.top_k_logits-Tuple{AbstractArray, Any}","page":"Home","title":"PPLM.top_k_logits","text":"top_k_logits(logits::AbstractArray, k; prob = false)\n\nMasks everything but the k top entries as -infinity (1e10). Incase of probs=true, everthing except top-k probabilities are masked to 0.0. logits is expected to be a vector.\n\n\n\n\n\n","category":"method"},{"location":"#PPLM.truncate_-Tuple{Any, Integer}","page":"Home","title":"PPLM.truncate_","text":"truncate_(x, max_length::Integer)\n\nTruncate the data to minimum of max_length and length of x.\n\n\n\n\n\n","category":"method"}]
}
