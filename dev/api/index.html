<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · PPLM.jl</title><link rel="canonical" href="https://adarshkumar712.github.io/PPLM.jl/api/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">PPLM.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../how/">How does it work?</a></li><li><a class="tocitem" href="../gpt2/">GPT2: Tokenization and Generation</a></li><li><a class="tocitem" href="../bow/">Bag Of Words Model</a></li><li><a class="tocitem" href="../discrim/">Discriminator Model</a></li><li><a class="tocitem" href="../discrim_train/">Discriminator Training</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#GPT2-Tokenizer"><span>GPT2 Tokenizer</span></a></li><li><a class="tocitem" href="#Discriminator-Model"><span>Discriminator Model</span></a></li><li><a class="tocitem" href="#Bag-of-Words"><span>Bag of Words</span></a></li><li><a class="tocitem" href="#Generation"><span>Generation</span></a></li><li><a class="tocitem" href="#Utils"><span>Utils</span></a></li></ul></li><li><a class="tocitem" href="../contact/">Contact Info</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/adarshkumar712/PPLM.jl/blob/master/docs/src/api.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Functions"><a class="docs-heading-anchor" href="#API-Functions">API Functions</a><a id="API-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#API-Functions" title="Permalink"></a></h1><p>Here are some of the API functions provided with this package:</p><h2 id="GPT2-Tokenizer"><a class="docs-heading-anchor" href="#GPT2-Tokenizer">GPT2 Tokenizer</a><a id="GPT2-Tokenizer-1"></a><a class="docs-heading-anchor-permalink" href="#GPT2-Tokenizer" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="PPLM.load_pretrained_tokenizer" href="#PPLM.load_pretrained_tokenizer"><code>PPLM.load_pretrained_tokenizer</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">load_pretrained_tokenizer(ty::Type{T}; unk_token=&quot;&lt;|endoftext|&gt;&quot;, eos_token=&quot;&lt;|endoftext|&gt;&quot;, pad_token=&quot;&lt;|endoftext|&gt;&quot;) where T&lt;:PretrainedTokenizer</code></pre><p>Load GPT2 tokenizer using Datadeps for pretrained bpe and vocab. Returns tokenizer as <code>GPT2Tokenizer</code> structure.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/tokenizer.jl#L39-L44">source</a></section><section><div><pre><code class="language-none">load_pretrained_tokenizer(path_bpe, path_vocab, unk_token, eos_token, pad_token)</code></pre><p>Load pretrained tokenizer for GPT2 from provided bpe and vocab file path. Initialises <code>unk_token</code>, <code>eos_token</code>, <code>pad_token</code> as provided with the function. Returns tokenizer as <code>GPT2Tokenizer</code> structure.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/tokenizer.jl#L51-L56">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.tokenize" href="#PPLM.tokenize"><code>PPLM.tokenize</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">tokenize(t::GPT2Tokenizer, text::AbstractString)</code></pre><p>Function to tokenize given <code>text</code> with tokenizer bpe encoder (<code>t.bpe_encode</code>). Returns a string vector of tokens.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/tokenizer.jl#L67-L71">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.encode" href="#PPLM.encode"><code>PPLM.encode</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">encode(t::GPT2Tokenizer, text::AbstractString; add_prefix_space=false)</code></pre><p>Returns the encoded vector of tokens (mapping from vocab of Tokenizer) for <code>text</code>. If <code>add_prefix_space</code>=true, add space at the start of &#39;text&#39; before tokenization. </p><p><strong>Example</strong></p><p>For single text:</p><pre><code class="language-julia">encode(tokenizer, text)</code></pre><p>For vector of text:</p><pre><code class="language-julia">map(x-&gt;encode(tokenizer, x), text_vector) </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/tokenizer.jl#L76-L94">source</a></section><section><div><pre><code class="language-none">encode(t::GPT2Tokenizer, tokens::Vector{String})</code></pre><p>Function to encode tokens vectors to their integer mapping from vocab of tokenizer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/tokenizer.jl#L103-L107">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.decode" href="#PPLM.decode"><code>PPLM.decode</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">decode(vocab::Vocabulary{T}, is::Vector{Int}) where T</code></pre><p>Return decoded vector of <code>string</code> tokens from the indices vector <code>is</code>, using the vocab.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/tokenizer.jl#L143-L147">source</a></section><section><div><pre><code class="language-none">decode(t::GPT2Tokenizer, tokens_ids::Vector{Int})</code></pre><p>Return decoded vector of <code>string</code> tokens from the indices vector <code>tokens_ids</code>, using the tokenizer <code>t</code> encoder .</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/tokenizer.jl#L158-L162">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.detokenize" href="#PPLM.detokenize"><code>PPLM.detokenize</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">detokenize(t::GPT2Tokenizer, tokens::Vector{String})</code></pre><p>BPE Decode the vector of strings, using the tokenizer <code>t</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/tokenizer.jl#L167-L171">source</a></section><section><div><pre><code class="language-none">detokenize(t::GPT2Tokenizer, tokens_ids::Vector{Int})</code></pre><p>Decode and Detokenize the vector of indices <code>token_ids</code>. Returns the final sentence after detokenization.</p><p><strong>Example</strong></p><p>For single vector of token_ids:</p><pre><code class="language-julia">detokenize(tokenizer, token_ids)</code></pre><p>For vector of vector of <code>token_ids</code>, use:</p><pre><code class="language-julia">map(x-&gt;decode(tokenizer, x), tokens_id_vector_of_vector)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/tokenizer.jl#L177-L195">source</a></section></article><h2 id="Discriminator-Model"><a class="docs-heading-anchor" href="#Discriminator-Model">Discriminator Model</a><a id="Discriminator-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Discriminator-Model" title="Permalink"></a></h2><h3 id="General"><a class="docs-heading-anchor" href="#General">General</a><a id="General-1"></a><a class="docs-heading-anchor-permalink" href="#General" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="PPLM.ClassifierHead" href="#PPLM.ClassifierHead"><code>PPLM.ClassifierHead</code></a> — <span class="docstring-category">Type</span></header><section><div><p>struct ClassifierHead     linear<em>layer::Dense     embed</em>size::Int     class_size::Int end</p><p>Struct for ClassifiedHead, defined with a single linear layer and two paramters: embed<em>size-&gt; Size of Embedding, class</em>size-&gt;Number of classes.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/discriminator.jl#L11-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.get_discriminator" href="#PPLM.get_discriminator"><code>PPLM.get_discriminator</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">get_discriminator(model; load_from_pretrained=false, discrim=nothing, file_name=nothing, version=2, class_size::Int=1, embed_size::Int=768, path=nothing)</code></pre><p>Function to create discriminator based on provided model. Incase, <code>load_from_pretrained</code> is set to be true, loads ClassifierHead layer from pretrained models or <code>path</code> provided.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/discriminator.jl#L160-L165">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.save_classifier_head" href="#PPLM.save_classifier_head"><code>PPLM.save_classifier_head</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">save_classifier_head(cl_head; file_name=nothing, path=nothing, args=nothing, register_discrim=true, discrim_name=&quot;&quot;)</code></pre><p>Function to save the ClassifiedHead as a BSON once the training is complete, based on the path provided. In case path is set as nothing, it saves the discriminators in <code>./pretrained_discriminators</code> folder relative to the package directory.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/discriminator.jl#L177-L181">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.save_discriminator" href="#PPLM.save_discriminator"><code>PPLM.save_discriminator</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">save_discriminator(discrim, discrim_name=&quot;Custom&quot;; file_name=nothing, path=nothing, args=nothing)</code></pre><p>Function to save ClassifiedHead part of discriminator (by calling <code>save_classifier_head</code> function), which is the only trainable part of discriminator</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/discriminator.jl#L208-L212">source</a></section></article><h3 id="Data-Processing"><a class="docs-heading-anchor" href="#Data-Processing">Data Processing</a><a id="Data-Processing-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Processing" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="PPLM.pad_seq" href="#PPLM.pad_seq"><code>PPLM.pad_seq</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">pad_seq(batch::AbstractVector{T}, pad_token::Integer=0)</code></pre><p>Function to add pad tokens in shorter sequence, to make the length of each sequence equal to the <code>max_length</code> ( calculated as <code>max(map(length, batch))</code>) in the batch. Pad token defaults to <code>0</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/data_preprocess.jl#L5-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.get_mask" href="#PPLM.get_mask"><code>PPLM.get_mask</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">get_mask(seq::AbstractMatrix{T}, pad_token::Integer=0, embed_size::Integer=768)</code></pre><p>Function to create mask for sequences against padding, so as to inform the model, that some part of sequenece is padded and hence to be ignored.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/data_preprocess.jl#L22-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.data_preprocess" href="#PPLM.data_preprocess"><code>PPLM.data_preprocess</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">data_preprocess(data_x, data_y, classification_type::String=&quot;Binary&quot;, num_classes::Integer=2; args=nothing)</code></pre><p>Function to preprocess <code>data_x</code> and <code>data_y</code> along with creating mask for the data_x. </p><p>Preprocessing for <code>data_x</code> consist of padding with pad token (expected to be provided as <code>args.pad_token</code>).</p><p>Preprocessing for <code>data_y</code> consist of creating <code>onehotbach</code> for <code>data_y</code> (if <code>classification_type</code> is not &quot;Binary&quot;), for <code>1:num_classes</code> else reshape the data as <code>(1, length(data_y))</code> </p><p>Returns <code>data_x</code>, <code>data_y</code>, <code>mask</code> after pre-processing.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/data_preprocess.jl#L34-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.load_data" href="#PPLM.load_data"><code>PPLM.load_data</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">load_data(data_x, data_y, tokenizer::PretrainedTokenizer;  batchsize::Integer=8, truncate::Bool=false, max_length::Integer=256, shuffle::Bool=false, drop_last::Bool=false, add_eos_start::Bool=true)</code></pre><p>Returns DataLoader for the <code>data_x</code> and <code>data_y</code> after processing the data<em>x, with batchsize=<code>batchsize</code>. The processing consist of tokenization of data</em>x and further truncation to <code>max_len</code> if <code>truncate</code> is set to be true. </p><p>If <code>add_eos_start</code> is set to true, add EOS token of tokenizer to the start. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/data_preprocess.jl#L74-L81">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.load_cached_data" href="#PPLM.load_cached_data"><code>PPLM.load_cached_data</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">load_cached_data(discrim::Union{DiscriminatorV1, DiscriminatorV2}, data_x, data_y, tokenizer::PretrainedTokenizer; truncate::Bool=false, max_length::Integer=256, shuffle::Bool=false, batchsize::Int=4, drop_last::Bool=false, classification_type=&quot;Binary&quot;, num_classes=2, args=nothing)</code></pre><p>Returns a DataLoader with (x, y) which can directly be feeded into classifier layer for training. </p><p>The function first loads the data using <a href="#PPLM.load_data"><code>load_data</code></a> function with batchsize=1, then passes each batch to the transformer model of <code>discrim</code> after data preprocessing, and then the average representation of the <code>hidden_states</code> are stored in a vector, which are then further loaded into a DataLoader, ready to use for classification training. </p><p><strong>Note</strong>: This functions saves time by cacheing the average representation of hidden states beforehand, avoiding passing the data through model in each epoch of training. This can be done as the model itself is <code>non-trainable</code> while training discriminator classifier head.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/data_preprocess.jl#L94-L103">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.load_data_from_csv" href="#PPLM.load_data_from_csv"><code>PPLM.load_data_from_csv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">load_data_from_csv(path_to_csv; text_col=&quot;text&quot;, label_col=&quot;label&quot;, delim=&#39;,&#39;, header=1)</code></pre><p>Load the data from a csv file based on the specified <code>text_col</code> column for text and <code>label_col</code> for target label. Returns vectors for <code>text</code> and <code>label</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/data_preprocess.jl#L140-L144">source</a></section></article><h3 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="PPLM.train!" href="#PPLM.train!"><code>PPLM.train!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">train!(discrim, data_loader; args=args)</code></pre><p>Train the discriminator using the provided <code>data_loader</code> training data and arguments <code>args</code> provided.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/discrim_train.jl#L43-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.test!" href="#PPLM.test!"><code>PPLM.test!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">test!(discrim, data_loader; args=nothing)</code></pre><p>Test the discriminator on test data provided using <code>data_loader</code>, based on Accuracy and NLL Loss. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/discrim_train.jl#L96-L101">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.train_discriminator" href="#PPLM.train_discriminator"><code>PPLM.train_discriminator</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">train_discriminator(text, labels, batchsize::Int=8, classification_type::String=&quot;Binary&quot;, num_classes::Int=2; model=&quot;gpt2&quot;, cached::Bool=true, discrim=nothing, tokenizer=nothing, truncate=true, max_length=256, train_size::Float64=0.9, lr::Float64=1e-5, epochs::Int=10, args=nothing)</code></pre><p>Function to train discriminator for provided <code>text</code> and target <code>labels</code>, based on set of function paramters provided. Returns <code>discrim</code> discriminator after training.</p><p>Here the <code>cached=true</code> allows cacheing of contexualized embeddings (forward pass) in GPT2 model, as the model itself is non-trainable. This reduces the time of training effectively as the forward pass through GPT2 model is to be done only once.</p><p><strong>Example</strong></p><p>Consider a Multiclass classification problem with class size of 5, it can trained on <code>text</code> and <code>labels</code> vectors using:</p><pre><code class="language-julia">train_discriminator(text, labels, 16, &quot;Multiclass&quot;, 5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/discrim_train.jl#L152-L167">source</a></section></article><h2 id="Bag-of-Words"><a class="docs-heading-anchor" href="#Bag-of-Words">Bag of Words</a><a id="Bag-of-Words-1"></a><a class="docs-heading-anchor-permalink" href="#Bag-of-Words" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="PPLM.get_bow_indices" href="#PPLM.get_bow_indices"><code>PPLM.get_bow_indices</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">get_bow_indices(bow_key_or_path_list::Vector{String}, tokenizer)</code></pre><p>Returns a list of <code>list of indices</code> of words from each Bag of word in the <code>bow_key_or_path_list</code>, after tokenization. The functions looks for provided BoW key in the registered artifacts <code>Artifacts.toml</code> file. In case not present there, function expects that bow_key is provided as the complete path to the file the URL to download .txt file.</p><p><strong>Example</strong></p><pre><code class="language-julia">get_bow_indices([&quot;legal&quot;, &quot;military&quot;])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/bow.jl#L3-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.build_bow_ohe" href="#PPLM.build_bow_ohe"><code>PPLM.build_bow_ohe</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">build_bow_ohe(bow_indices, tokenizer)</code></pre><p>Build and return a list of <code>one_hot_matrix</code> for each Bag Of Words list from indices. Each item of the list is of dimension <code>(num_of_BoW_words, tokenizer.vocab_size)</code>. </p><p>Note: While building the OHE of word indices, it only keeps those words, which have length <code>1</code> after tokenization and discard the rest.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/bow.jl#L39-L46">source</a></section></article><h2 id="Generation"><a class="docs-heading-anchor" href="#Generation">Generation</a><a id="Generation-1"></a><a class="docs-heading-anchor-permalink" href="#Generation" title="Permalink"></a></h2><h3 id="Normal"><a class="docs-heading-anchor" href="#Normal">Normal</a><a id="Normal-1"></a><a class="docs-heading-anchor-permalink" href="#Normal" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="PPLM.sample_normal" href="#PPLM.sample_normal"><code>PPLM.sample_normal</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">sample_normal(;prompt=&quot;I hate the customs&quot;, tokenizer=nothing, model=nothing, max_length=100, method=&quot;top_k&quot;, k=50, t=1.2, p=0.5, add_eos_start=true)</code></pre><p>Function to generate normal Sentences with <code>model</code> and <code>tokenizer</code> provided. In case not provided, function itself create instance of GPT2-small tokenizer and LM Head Model. The sentences are started with the provided <code>prompt</code>, and generated till token length reaches <code>max_length</code>.</p><p>Two sampling methods of generation are provided with this function:</p><ol><li>method=&#39;top_k&#39;</li><li>method=&#39;nucleus&#39;</li></ol><p>Any of these methods can be used provided with either k or p.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/generate.jl#L5-L16">source</a></section></article><h3 id="PPLM"><a class="docs-heading-anchor" href="#PPLM">PPLM</a><a id="PPLM-1"></a><a class="docs-heading-anchor-permalink" href="#PPLM" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="PPLM.sample_pplm" href="#PPLM.sample_pplm"><code>PPLM.sample_pplm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">function sample_pplm(pplm_args;tokenizer=nothing, model=nothing, prompt=&quot;I hate the customs&quot;, add_eos_start=true)</code></pre><p>Function for PPLM model based generation. Generate perturbed sentence using <code>pplm_args</code>, tokenizer and model (GPT2, in case not provided), starting with <code>prompt</code>. In this function the generation is based on the arguments/parameters provided in <code>pplm_args</code>, which is an instance of <code>pplm</code> struct.  </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/generate.jl#L52-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.perturb_probs" href="#PPLM.perturb_probs"><code>PPLM.perturb_probs</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">perturb_probs(probs, tokenizer, args)</code></pre><p>Perturb probabilities <code>probs</code> based on provided Bag of Words list (as given with <code>args</code>). This function is supported only for BoW model. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/pplm.jl#L63-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.perturb_hidden_bow" href="#PPLM.perturb_hidden_bow"><code>PPLM.perturb_hidden_bow</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">perturb_hidden_bow(hidden, model, tokenizer, args)</code></pre><p>Perturb hidden states <code>hidden</code> based on provided Bag of Words list (as given with <code>args</code>). The perturbation is primarily based on the gradient calculated over losses evaluated over desired Bag of Words and KL Divergence from original token. </p><p>Also checkout <a href="#PPLM.perturb_hidden_discrim"><code>perturb_hidden_discrim</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/pplm.jl#L103-L109">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.perturb_past_bow" href="#PPLM.perturb_past_bow"><code>PPLM.perturb_past_bow</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">perturb_past_bow(model, prev, past, original_probs, args)</code></pre><p>Perturb past key values <code>prev</code> based on provided Bag of Words list (as given with <code>args</code>). The perturbation is primarily based on the gradient calculated over losses evaluated over desired Bag of Words and KL Divergence from original token. </p><p>Also checkout <a href="#PPLM.perturb_past_discrim"><code>perturb_past_discrim</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/pplm.jl#L147-L153">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.perturb_hidden_discrim" href="#PPLM.perturb_hidden_discrim"><code>PPLM.perturb_hidden_discrim</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">perturb_hidden_discrim(hidden, model, tokenizer, args)</code></pre><p>Perturb hidden states <code>hidden</code> based on provided Discriminator (as given with <code>args</code>). The perturbation is primarily based on the gradient calculated over losses evaluated over desired Discriminator attribute and KL Divergence from original token. </p><p>Also checkout <a href="#PPLM.perturb_hidden_bow"><code>perturb_hidden_bow</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/pplm.jl#L196-L202">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.perturb_past_discrim" href="#PPLM.perturb_past_discrim"><code>PPLM.perturb_past_discrim</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">perturb_past_discrim(model, prev, past, original_probs, args)</code></pre><p>Perturb past key values <code>prev</code> based on provided Discriminator (as given with <code>args</code>). The perturbation is primarily based on the gradient calculated over losses evaluated over desired Discriminator attribute and KL Divergence from original token.</p><p>Also checkout <a href="#PPLM.perturb_past_bow"><code>perturb_past_bow</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/pplm.jl#L243-L249">source</a></section></article><h2 id="Utils"><a class="docs-heading-anchor" href="#Utils">Utils</a><a id="Utils-1"></a><a class="docs-heading-anchor-permalink" href="#Utils" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="PPLM.get_gpt2" href="#PPLM.get_gpt2"><code>PPLM.get_gpt2</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">get_gpt2()</code></pre><p>Function to load gpt2 lmheadmodel along with the <code>tokenizer</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/PPLM.jl#L37-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.get_gpt2_medium" href="#PPLM.get_gpt2_medium"><code>PPLM.get_gpt2_medium</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">get_gpt2_medium()</code></pre><p>Function to load gpt2-medium lmhead model along with the <code>tokenizer</code>.</p><p><strong>Note</strong>: In case this function gives error of permission denied, try changing the file permissions for the Artifacts.toml file of Transformers.jl package (as it is read only by default) under the <code>src/huggingface</code> folder. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/PPLM.jl#L48-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.set_device" href="#PPLM.set_device"><code>PPLM.set_device</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">set_device(d_id=0)</code></pre><p>Function to set cuda device if available and also to disallow scalar operations</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/PPLM.jl#L64-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.get_registered_file" href="#PPLM.get_registered_file"><code>PPLM.get_registered_file</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">get_registered_file(name)</code></pre><p>Fetch registered file path from Artifacts.toml, based on the artifact <code>name</code>.  </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/artifacts.jl#L5-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.get_artifact" href="#PPLM.get_artifact"><code>PPLM.get_artifact</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">get_artifact(name)</code></pre><p>Utility function to download/install the artifact in case not already installed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/artifacts.jl#L23-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.register_custom_file" href="#PPLM.register_custom_file"><code>PPLM.register_custom_file</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">register_custom_file(artifact_name, file_name, path)</code></pre><p>Function to register custom file under <code>artifact_name</code> in Artifacts.toml. <code>path</code> expects path of the directory where the file <code>file_name</code> is stored. Stores the complete path to the file as Artifact URL.</p><p><strong>Example</strong></p><pre><code class="language-none">register_custom_file(&#39;custom&#39;, &#39;xyz.txt&#39;,&#39;./folder/folder/&#39;)
</code></pre><p>Note: In case this gives permission denied error, change the Artifacts.toml file permissions using  <code>chmod(path_to_file_in_julia_installation , 0o764)</code>or similar.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/artifacts.jl#L40-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.top_k_sample" href="#PPLM.top_k_sample"><code>PPLM.top_k_sample</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">top_k_sample(probs; k=10)</code></pre><p>Sampling function to return index from <code>top_k</code> probabilities, based on provided <code>k</code>. Function removes all tokens with a probability less than the last token of the top_k before sampling.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/utils.jl#L38-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.nucleus_sample" href="#PPLM.nucleus_sample"><code>PPLM.nucleus_sample</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">nucleus_sample(probs; p=0.8)</code></pre><p>Nuclues sampling function, to return after sampling reverse sorted probabilities <code>probs</code> till the index, where cumulative probability remains less than provided <code>p</code>. It removes tokens with cumulative probability above the threshold <code>p</code> before sampling.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/utils.jl#L52-L56">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.binary_accuracy" href="#PPLM.binary_accuracy"><code>PPLM.binary_accuracy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">binary_accuracy(y_pred, y_true; threshold=0.5)</code></pre><p>Calculates Averaged Binary Accuracy based on <code>y_pred</code> and <code>y_true</code>. Argument <code>threshold</code> is used to specify the minimum predicted probability <code>y_pred</code> required to be labelled as <code>1</code>. Default value set as <code>0.5</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/utils.jl#L79-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="PPLM.categorical_accuracy" href="#PPLM.categorical_accuracy"><code>PPLM.categorical_accuracy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">categorical_accuracy(y_pred, y_true)</code></pre><p>Calculates Averaged Categorical Accuracy based on <code>y_pred</code> and <code>y_true</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/adarshkumar712/PPLM.jl/blob/0675c12a3550730da1259d95aa172351fc35f984/src/utils.jl#L89-L93">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../discrim_train/">« Discriminator Training</a><a class="docs-footer-nextpage" href="../contact/">Contact Info »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 11 August 2021 12:55">Wednesday 11 August 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
