<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>GPT2: Tokenization and Generation · PPLM.jl</title><link rel="canonical" href="https://adarshkumar712.github.io/PPLM.jl/gpt2/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">PPLM.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../how/">How does it work?</a></li><li class="is-active"><a class="tocitem" href>GPT2: Tokenization and Generation</a><ul class="internal"><li><a class="tocitem" href="#Tokenization"><span>Tokenization</span></a></li><li><a class="tocitem" href="#Generation-:-Normal-Text"><span>Generation : Normal Text</span></a></li></ul></li><li><a class="tocitem" href="../bow/">Bag Of Words Model</a></li><li><a class="tocitem" href="../discrim/">Discriminator Model</a></li><li><a class="tocitem" href="../discrim_train/">Discriminator Training</a></li><li><a class="tocitem" href="../api/">API</a></li><li><a class="tocitem" href="../contact/">Contact Info</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>GPT2: Tokenization and Generation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>GPT2: Tokenization and Generation</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/adarshkumar712/PPLM.jl/blob/master/docs/src/gpt2.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="GPT2:-Tokenization-and-Generation"><a class="docs-heading-anchor" href="#GPT2:-Tokenization-and-Generation">GPT2: Tokenization and Generation</a><a id="GPT2:-Tokenization-and-Generation-1"></a><a class="docs-heading-anchor-permalink" href="#GPT2:-Tokenization-and-Generation" title="Permalink"></a></h1><p>PPLM.jl comes along with support of GPT2 tokenizer and Generation. Here is an example of how you can use this feature of PPLM.jl package for tokenization and normal generation</p><h2 id="Tokenization"><a class="docs-heading-anchor" href="#Tokenization">Tokenization</a><a id="Tokenization-1"></a><a class="docs-heading-anchor-permalink" href="#Tokenization" title="Permalink"></a></h2><p>PPLM.jl allows users to load pre-trained GPT2 tokenizers based on BytePairEncoding.jl and Transformers.jl, which can then be used to tokenize/encode English text with a single line of code. The tokenizer is implemented with the following structure:</p><pre><code class="language-julia">abstract type GPT2 &lt;: PretrainedTokenizer end

struct GPT2Tokenizer &lt;: GPT2
    encoder::Vocabulary{String}
    bpe_encode::GenericBPE
    bpe_decode::UnMap
    vocab::Dict{String, Any}
    unk_token::String
    unk_id::Int   
    eos_token::String
    eos_token_id::Int
    pad_token::String
    pad_token_id::Int
end</code></pre><h4 id="Example-of-Tokenization"><a class="docs-heading-anchor" href="#Example-of-Tokenization">Example of Tokenization</a><a id="Example-of-Tokenization-1"></a><a class="docs-heading-anchor-permalink" href="#Example-of-Tokenization" title="Permalink"></a></h4><p>Let&#39;s see how you can tokenize text with PPLM.</p><pre><code class="language-julia"># Load Tokenizer
using PPLM
tokenizer = PPLM.load_pretrained_tokenizer(GPT2)

sentence = &quot;This is an example of Tokenization&quot;</code></pre><p>Once, you have loaded your tokenizer, one can use either of the methods:</p><pre><code class="language-julia">tokens = tokenizer(sentence)
# or 
tokens = encode(tokenizer, sentence)</code></pre><p>It will return the following output:</p><pre><code class="language-none">7-element Vector{Int64}:
  1213
   319
   282
  1673
   287
 29131
  1635</code></pre><p>Now you have your list of tokens. Suppose you want to get back your sentence. This can be done in two ways:</p><pre><code class="language-none"># Firsth Method:
sentence = detokenize(tokenizer, tokens)

# Second Method:
decoded_tokens_list = decode(tokenizer, tokens)	
# returns vector: [&quot;This&quot;, &quot;Ġis&quot;, &quot;Ġan&quot;, &quot;Ġexample&quot;, &quot;Ġof&quot;, &quot;ĠToken&quot;, &quot;ization&quot;]
sentence = detokenizer(tokenizer, decoded_tokens_list) </code></pre><p>You will get back your original sentence <code>This is an example of Tokenization</code></p><h2 id="Generation-:-Normal-Text"><a class="docs-heading-anchor" href="#Generation-:-Normal-Text">Generation : Normal Text</a><a id="Generation-:-Normal-Text-1"></a><a class="docs-heading-anchor-permalink" href="#Generation-:-Normal-Text" title="Permalink"></a></h2><p>PPLM.jl can be used to generate normal (unperturbed) text with the GPT2 model, with any of the two sampling methods <code>top_k</code> and <code>nucleus</code>:</p><p>To generate text, you can use the following code:</p><p>Here is a Sample text generated with GPT2 using the above code:</p><blockquote><p>With <strong>Top_k sampling</strong>, k=50, prompt = &quot;Fruits are&quot;</p></blockquote><pre><code class="language-julia">&quot;Fruits are the key ingredient in our diet; their vitamins, and proteins are essential to build our immune system. What makes a good fruit one of them is simply as simple as your diet is. Fruit is one simple nutrient that is used effectively as a defense against sickness and stress (which can be very life changing indeed). When the body has just consumed enough fat for at least 40-50 days, the body also releases hormones known as the hormone estrogen in order to prevent infection. A good diet makes life easier&quot;</code></pre><blockquote><p>With <strong>Nucleus sampling</strong>, p=0.6, prompt = &quot;Fruits are&quot;</p></blockquote><pre><code class="language-julia">&quot;Fruits are packed with the goodness of ancient Greek life, plants that protect and revive us from death. At every stone in the garden, your fruit may reflect on the people who once carried you from town to town, those who would still give you food to live, and the perfect pair of hand-gloved fingers you may wear in your golden bedroll.&quot;</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../how/">« How does it work?</a><a class="docs-footer-nextpage" href="../bow/">Bag Of Words Model »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 10 August 2021 12:23">Tuesday 10 August 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
