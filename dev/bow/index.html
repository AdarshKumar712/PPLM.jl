<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Bag Of Words Model · PPLM.jl</title><link rel="canonical" href="https://adarshkumar712.github.io/PPLM.jl/bow/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">PPLM.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../how/">How does it work?</a></li><li><a class="tocitem" href="../gpt2/">GPT2: Tokenization and Generation</a></li><li class="is-active"><a class="tocitem" href>Bag Of Words Model</a><ul class="internal"><li><a class="tocitem" href="#Perturb-Probability"><span>Perturb Probability</span></a></li><li><a class="tocitem" href="#Perturb-Hidden-States"><span>Perturb Hidden States</span></a></li></ul></li><li><a class="tocitem" href="../discrim/">Discriminator Model</a></li><li><a class="tocitem" href="../discrim_train/">Discriminator Training</a></li><li><a class="tocitem" href="../api/">API</a></li><li><a class="tocitem" href="../contact/">Contact Info</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Bag Of Words Model</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Bag Of Words Model</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/adarshkumar712/PPLM.jl/blob/master/docs/src/bow.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Bag-Of-Words-Model"><a class="docs-heading-anchor" href="#Bag-Of-Words-Model">Bag Of Words Model</a><a id="Bag-Of-Words-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Bag-Of-Words-Model" title="Permalink"></a></h1><p>In the BoW model, we have a list of words that correspond to that particular attribute, usually used in sentences that can be considered to have that attribute. For example, consider the case of Topic-based attribute, let&#39;s say <code>Politics</code>, where we want to drive the topic of generation towards politics, then a typical BagOfWord for <code>Politics</code> will include words like government, politics, democracy, federation, etc, etc. These words will then be used for perturbation through PPLM.jl. Before getting into how to do this with PPLM.jl, let&#39;s first delve into some technical background and prerequisites of the BoW model and how it works</p><p>Let&#39;s first initialize the package and model.</p><pre><code class="language-julia">using PPLM

tokenizer, model = PPLM.get_gpt2();</code></pre><blockquote><p><strong>Prompt</strong>: To conclude</p></blockquote><h2 id="Perturb-Probability"><a class="docs-heading-anchor" href="#Perturb-Probability">Perturb Probability</a><a id="Perturb-Probability-1"></a><a class="docs-heading-anchor-permalink" href="#Perturb-Probability" title="Permalink"></a></h2><p>This feature only supports for Bag of Words Model. Perturbation of probability can be done similar to the given example:</p><pre><code class="language-julia">args= PPLM.pplm(perturb=&quot;probs&quot;, bow_list = [&quot;politics&quot;], stepsize=0.1, fusion_gm_scale=0.8f0, top_k=50)

PPLM.sample_pplm(args; tokenizer=tokenizer, model=model, prompt=&quot;To conclude&quot;)
</code></pre><p>Another more crude way of generation could be:</p><pre><code class="language-julia">input_ = [tokenizer.eos_token_id; tokenizer(&quot;To conclude&quot;)]

args= PPLM.pplm(perturb=&quot;probs&quot;, bow_list = [&quot;politics&quot;], stepsize=0.1, fusion_gm_scale=0.8f0, top_k=50)

for i in 1:100
    input_ids = reshape(input_[:], :, 1)
    outputs = model(input_ids; output_attentions=false,
                        output_hidden_states=true,
                        use_cache=false);
    original_logits = outputs.logits[:, end, 1]
    original_probs = PPLM.temp_softmax(original_logits; t=args.temperature)
    pert_probs = perturb_probs(original_probs, tokenizer, args3)
    gm_scale = args.fusion_gm_scale
    pert_probs = Float32.((original_probs.^(1-gm_scale)).*(pert_probs.^(gm_scale)))
    new_token = PPLM.top_k_sample(pert_probs; k=args.top_k)[1]
    push!(input_, new_token)
end

text = detokenize(tokenizer, input_)
</code></pre><p>Sample generation:</p><pre><code class="language-julia">&quot;To conclude the last week about our current policy, they say we \&quot;don&#39;t follow their dictates.\&quot;[25][26] 
We&#39;ve never followed that precept, so in order, what we have is a very limited, almost not always followed 
agenda. It took decades to implement and it has already occurred once.[27] These are the arguments that 
most conservative leaders used to put forth to show the world that our public spending has failed – as 
conservatives pointed out. What they don&#39;t seem to understand is that this ideology&quot;</code></pre><h2 id="Perturb-Hidden-States"><a class="docs-heading-anchor" href="#Perturb-Hidden-States">Perturb Hidden States</a><a id="Perturb-Hidden-States-1"></a><a class="docs-heading-anchor-permalink" href="#Perturb-Hidden-States" title="Permalink"></a></h2><p>Perturbation of hidden states can be done similar to the given example</p><pre><code class="language-julia">args= PPLM.pplm(perturb=&quot;hidden&quot;, bow_list = [&quot;politics&quot;], stepsize=0.1, fusion_gm_scale=0.8f0, top_k=50)

PPLM.sample_pplm(args; tokenizer=tokenizer, model=model, prompt=&quot;To conclude&quot;)
</code></pre><p>Another more crude way of generation could be:</p><pre><code class="language-julia">
input_ = [tokenizer.eos_token_id; tokenizer(&quot;To conclude&quot;)]

args= PPLM.pplm(perturb=&quot;hidden&quot;, bow_list = [&quot;politics&quot;], stepsize=0.1, fusion_gm_scale=0.8f0, top_k=50)

for i in 1:100
    input_ids = reshape(input_[:], :, 1) |&gt; gpu
    outputs = model(input_ids; output_attentions=false,
                        output_hidden_states=true,
                        use_cache=false);
    original_logits = outputs.logits[:, end, 1]
    original_probs = PPLM.temp_softmax(original_logits; t=args.temperature)
    
    hidden = outputs.hidden_states[end]
    
    modified_hidden = perturb_hidden_bow(model, hidden, args3)
    pert_logits = model.lm_head(modified_hidden)[:, end, 1]
    pert_probs = PPLM.temp_softmax(pert_logits; t=args.temperature)
    
    gm_scale = args.fusion_gm_scale
    pert_probs = Float32.((original_probs.^(1-gm_scale)).*(pert_probs.^(gm_scale))) |&gt; cpu
    new_token = PPLM.top_k_sample(pert_probs; k=args.top_k)[1]
    push!(input_, new_token)
    #print(&quot;.&quot;)
end

text = detokenize(tokenizer, input_)
</code></pre><p>Sample generation:</p><pre><code class="language-julia">&quot;To conclude this brief essay, I have briefly discussed one of my own writing&#39;s main points: How a great 
many poor working people who were forced by the government to sell goods to high-end supermarkets to make 
ends meet were put off purchasing goods at a time they wouldn&#39;t be able afford. That point of distinction 
arises in every social democracy I identify as libertarian.\n\nA large number of people in this group 
simply did not follow basic political norms, and in order not to lose faith that politics was in&quot;</code></pre><h3 id="Perturb-Past"><a class="docs-heading-anchor" href="#Perturb-Past">Perturb Past</a><a id="Perturb-Past-1"></a><a class="docs-heading-anchor-permalink" href="#Perturb-Past" title="Permalink"></a></h3><pre><code class="language-julia">args= PPLM.pplm(perturb=&quot;past&quot;, bow_list = [&quot;politics&quot;], stepsize=0.1, fusion_gm_scale=0.8f0, top_k=50)

PPLM.sample_pplm(args; tokenizer=tokenizer, model=model, prompt=&quot;To conclude&quot;)
</code></pre><p>Another more crude way of generation could be:</p><pre><code class="language-julia">input_ = [tokenizer.eos_token_id; tokenizer(&quot;To conclude&quot;)]

args= PPLM.pplm(perturb=&quot;past&quot;, bow_list = [&quot;politics&quot;], stepsize=0.1, fusion_gm_scale=0.8f0, top_k=50)

for i in 1:100
    input_ids = reshape(input_[:], :, 1) |&gt; gpu
    inp = input_ids[1:end-1, :]
    prev = input_ids[end:end, :]
    outputs = model(inp; output_attentions=false,
                        output_hidden_states=true,
                        use_cache=true);
    past = outputs.past_key_values;
    original_logits = outputs.logits[:, end, 1]
    original_probs = PPLM.temp_softmax(original_logits; t=args.temperature)
    
    new_past = perturb_past_bow(model, prev, past, original_probs, args3)
    output_new = model(prev; past_key_values=new_past,
                                        output_attentions=false,
                                        output_hidden_states=true,
                                        use_cache=true);    
    pert_logits = output_new.logits[:, end, 1]
    pert_probs = PPLM.temp_softmax(pert_logits; t=args.temperature)
    
    gm_scale = args.fusion_gm_scale
    pert_probs = Float32.((original_probs.^(1-gm_scale)).*(pert_probs.^(gm_scale))) |&gt; cpu
    new_token = PPLM.top_k_sample(pert_probs; k=args.top_k)[1]
    push!(input_, new_token)
    #print(&quot;.&quot;)
end

text = detokenize(tokenizer, input_)
</code></pre><p>Sample generation:</p><pre><code class="language-julia">&quot;To conclude, it&#39;s important for governments, from the government of Canada, who decide matters of 
international importance and culture and language issues to the authorities the responsible party for 
immigration enforcement, when that person&#39;s an international terrorist, as these are important and cultural 
communities, rather and international business people, like the Canadian government, should take seriously 
when they say these, to the authorities, and then have the Canadian people deal with, and to them be more 
involved in the process itself and their work ethics should really be to&quot;</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../gpt2/">« GPT2: Tokenization and Generation</a><a class="docs-footer-nextpage" href="../discrim/">Discriminator Model »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 10 August 2021 13:58">Tuesday 10 August 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
